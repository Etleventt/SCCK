# lightning.pytorch==2.0.9.post0
seed_everything: 42

trainer:
  accelerator: gpu
  strategy: ddp_find_unused_parameters_false
  devices: [0]                 # 单卡请改为 [0]，并将 sync_batchnorm 设为 false
  num_nodes: 1
  precision: 32
  logger:
    - class_path: CSVLogger
      init_args:
        save_dir: logs
        name: experiment_ck_256
  callbacks:
    - class_path: lightning.pytorch.callbacks.TQDMProgressBar
      init_args:
        refresh_rate: 1
        # leave: true
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        every_n_epochs: 1         # 256 训练更重，降低检查点频率
        save_on_train_epoch_end: true
        save_top_k: -1
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
  max_epochs: 5000
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 0
  log_every_n_steps: 1
  deterministic: true
  inference_mode: true
  use_distributed_sampler: true   # 与 256_fair 对齐，val/test 不复用样本
  sync_batchnorm: false

model:
  lr_g: 0.00016
  lr_d: 0.0001                    # 无判别器时忽略
  disc_grad_penalty_freq: 10       # 忽略
  disc_grad_penalty_weight: 0.5    # 忽略
  lambda_rec_loss: 0.5
  optim_betas: [0.5, 0.9]
  eval_mask: false                 # 256 官方集无 mask，沿用 256_fair 设置
  eval_subject: true

  # —— 关闭判别器，只走无对抗分支 ——
  discriminator_params: null

  # —— 同时启用标准 SC 与 CK ——
  use_standard_sc: true
  sc_prob: 0.5                     # 保留无效占位
  sc_stop_grad: true               # 保留无效占位
  drop_r_prob: 0.0

  # —— CK 跨步一致性（t→t−1→t−2 vs 直接 t→t−2）——
  lambda_ck: 0.2                   # 建议 0.1~0.3 起步
  ck_prob: 0.5                     # 50% 迭代启用，额外一次前向
  ck_time_norm: true               # 用 1/|a_{t-1}| 做时间归一化
  ck_detach_tm1: true              # 在 t−1 的 G 输入上 stop-grad
  ck_shared_noise: true            # 共享噪声的采样版 CK（更匹配 t-1 的输入分布）
  ck_warmup_steps: 2000            # 前 2000 step 不启用 CK（先让主监督稳定）
  ck_t_lo_frac: 0.3                # CK 仅在中段时刻使用，避开极吵/极净
  ck_t_hi_frac: 0.9

  diffusion_params:
    n_steps: 10
    beta_start: 0.1
    beta_end: 3.0
    gamma: 1
    n_recursions: 2
    consistency_threshold: 0.0

  generator_params:
    self_recursion: true
    image_size: ${data.image_size}
    z_emb_dim: 256
    ch_mult: [1, 1, 2, 2, 4, 4]
    num_res_blocks: 2
    attn_resolutions: [16]
    dropout: 0.0
    resamp_with_conv: true
    conditional: true
    fir: true
    fir_kernel: [1, 3, 3, 1]
    skip_rescale: true
    resblock_type: biggan
    progressive: none
    progressive_input: residual
    embedding_type: positional
    combine_method: sum
    fourier_scale: 16
    nf: 64
    num_channels: 2
    nz: 100
    n_mlp: 3
    centered: true
    not_use_tanh: false

  # 如需对比“SC+CK”，可在命令行加：--model.use_standard_sc true

data:
  train_batch_size: 4
  val_batch_size: 4
  test_batch_size: 6

  dataset_dir: ./dataset/brats256_selfrdb_official
  source_modality: t2
  target_modality: flair
  dataset_class: NumpyDataset
  image_size: 256
  padding: true
  norm: true
  num_workers: 4
  # 训练按需懒加载，避免一次性读入全部切片
  train_lazy: true
  # --- 子集裁剪（加速验证/测试，可按需调整为 0 全量）---
  val_max_samples: 200
  val_subset_mode: random
  val_subset_seed: 42
  test_max_samples: 200
  test_subset_mode: random
  test_subset_seed: 42

ckpt_path: 
# /home/xiaobin/Projects/SelfRDB/logs/experiment_ck/version_8/checkpoints/epoch=14-step=12675.ckpt
